{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTIFIER: a\n",
      "OPERATOR: =\n",
      "INTEGER: 10\n",
      "IDENTIFIER: b\n",
      "OPERATOR: =\n",
      "INTEGER: 20\n",
      "KEYWORD: if\n",
      "IDENTIFIER: a\n",
      "OPERATOR: =\n",
      "OPERATOR: =\n",
      "IDENTIFIER: b\n",
      "KEYWORD: print\n",
      "KEYWORD: true\n",
      "KEYWORD: else\n",
      "KEYWORD: print\n",
      "KEYWORD: false\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class TokenType:\n",
    "    INTEGER = 'INTEGER'\n",
    "    BOOLEAN = 'BOOLEAN'\n",
    "    OPERATOR = 'OPERATOR'\n",
    "    ASSIGNMENT = 'ASSIGNMENT'\n",
    "    EQUALITY = 'EQUALITY'\n",
    "    INEQUALITY = 'INEQUALITY'\n",
    "    KEYWORD = 'KEYWORD'\n",
    "    IDENTIFIER = 'IDENTIFIER'\n",
    "    PRINT = 'PRINT'\n",
    "    TRUE = 'TRUE'\n",
    "    FALSE = 'FALSE'\n",
    "    COMMENT = 'COMMENT'\n",
    "    ERROR = 'ERROR'\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, token_type, lexeme):\n",
    "        self.token_type = token_type\n",
    "        self.lexeme = lexeme\n",
    "\n",
    "def scan(filename):\n",
    "    tokens = []\n",
    "    keywords = {'if', 'else', 'print', 'true', 'false'}\n",
    "    operators = {'+', '-', '*', '/', '=', '==', '!='}\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line_num, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith('//'):\n",
    "                tokens.append(Token(TokenType.COMMENT, line))\n",
    "                continue\n",
    "            tokens.extend(scan_line(line, line_num + 1, keywords, operators))\n",
    "    return tokens\n",
    "\n",
    "def scan_line(line, line_num, keywords, operators):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(line):\n",
    "        if line[i].isspace():\n",
    "            i += 1\n",
    "            continue\n",
    "        elif line[i].isdigit():\n",
    "            token = scan_integer(line, i, line_num)\n",
    "            tokens.append(token)\n",
    "            i = token_end_position(token, i)\n",
    "        elif line[i].isalpha() or line[i] == '_':\n",
    "            token = scan_identifier_or_keyword(line, i, line_num, keywords)\n",
    "            tokens.append(token)\n",
    "            i = token_end_position(token, i)\n",
    "        elif line[i] in operators:\n",
    "            tokens.append(Token(TokenType.OPERATOR, line[i]))\n",
    "            i += 1\n",
    "        elif line[i] == '=':\n",
    "            if i + 1 < len(line) and line[i + 1] == '=':\n",
    "                tokens.append(Token(TokenType.EQUALITY, '=='))\n",
    "                i += 2\n",
    "            else:\n",
    "                tokens.append(Token(TokenType.ASSIGNMENT, line[i]))\n",
    "                i += 1\n",
    "        elif line[i] == '!':\n",
    "            if i + 1 < len(line) and line[i + 1] == '=':\n",
    "                tokens.append(Token(TokenType.INEQUALITY, '!='))\n",
    "                i += 2\n",
    "            else:\n",
    "                tokens.append(Token(TokenType.ERROR, f\"Invalid symbol '!' at line {line_num}\"))\n",
    "                i += 1\n",
    "        else:\n",
    "            tokens.append(Token(TokenType.ERROR, f\"Invalid symbol '{line[i]}' at line {line_num}\"))\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "def scan_integer(line, start, line_num):\n",
    "    integer_pattern = re.compile(r'\\d+')\n",
    "    match = integer_pattern.match(line, start)\n",
    "    lexeme = match.group()\n",
    "    return Token(TokenType.INTEGER, lexeme)\n",
    "\n",
    "def scan_identifier_or_keyword(line, start, line_num, keywords):\n",
    "    identifier_pattern = re.compile(r'[a-zA-Z_]\\w*')\n",
    "    match = identifier_pattern.match(line, start)\n",
    "    lexeme = match.group()\n",
    "    if lexeme in keywords:\n",
    "        return Token(TokenType.KEYWORD, lexeme)\n",
    "    elif lexeme == 'print':\n",
    "        return Token(TokenType.PRINT, lexeme)\n",
    "    elif lexeme == 'true':\n",
    "        return Token(TokenType.TRUE, lexeme)\n",
    "    elif lexeme == 'false':\n",
    "        return Token(TokenType.FALSE, lexeme)\n",
    "    else:\n",
    "        return Token(TokenType.IDENTIFIER, lexeme)\n",
    "\n",
    "def token_end_position(token, start):\n",
    "    return start + len(token.lexeme)\n",
    "\n",
    "# Test the scanner\n",
    "filename = 'test.minilang'\n",
    "tokens = scan(filename)\n",
    "for token in tokens:\n",
    "    print(f\"{token.token_type}: {token.lexeme}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
